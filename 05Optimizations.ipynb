{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. A/B Testing\n",
    "2. Iterative Refining\n",
    "3. Evaluation Metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2\")\n",
    "\n",
    "\n",
    "# Define a helper function to generate responses\n",
    "def generate_response(prompt):\n",
    "    \"\"\"Generate a response using the language model.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input prompt.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response.\n",
    "    \"\"\"\n",
    "    return llm.invoke(prompt).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response(response, criteria):\n",
    "    \"\"\"Evaluate the quality of a response based on given criteria.\n",
    "\n",
    "    Args:\n",
    "        response (str): The generated response.\n",
    "        criteria (list): List of criteria to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        float: The average score across all criteria.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for criterion in criteria:\n",
    "        print(f\"Evaluating response based on {criterion}...\")\n",
    "        prompt = f\"On a scale of 1-10, rate the following response on {criterion}. Start your response with the numeric score:\\n\\n{response}\"\n",
    "        response = generate_response(prompt)\n",
    "        # show 50 characters of the response\n",
    "        # Use regex to find the first number in the response\n",
    "        score_match = re.search(r\"\\d+\", response)\n",
    "        if score_match:\n",
    "            score = int(score_match.group())\n",
    "            scores.append(min(score, 10))  # Ensure score is not greater than 10\n",
    "        else:\n",
    "            print(\n",
    "                f\"Warning: Could not extract numeric score for {criterion}. Using default score of 5.\"\n",
    "            )\n",
    "            scores.append(5)  # Default score if no number is found\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A/B Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt variations\n",
    "prompt_a = PromptTemplate(\n",
    "    input_variables=[\"topic\"], template=\"Explain {topic} in simple terms.\"\n",
    ")\n",
    "\n",
    "prompt_b = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Provide a beginner-friendly explanation of {topic}, including key concepts and an example.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform A/B test\n",
    "topic = \"machine learning\"\n",
    "response_a = generate_response(prompt_a.format(topic=topic))\n",
    "response_b = generate_response(prompt_b.format(topic=topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating response based on clarity...\n",
      "Evaluating response based on informativeness...\n",
      "Evaluating response based on engagement...\n",
      "Evaluating response based on clarity...\n",
      "Evaluating response based on informativeness...\n",
      "Evaluating response based on engagement...\n",
      "Prompt A score: 7.00\n",
      "Prompt B score: 8.00\n",
      "Winning prompt: B\n"
     ]
    }
   ],
   "source": [
    "criteria = [\"clarity\", \"informativeness\", \"engagement\"]\n",
    "score_a = evaluate_response(response_a, criteria)\n",
    "score_b = evaluate_response(response_b, criteria)\n",
    "\n",
    "print(f\"Prompt A score: {score_a:.2f}\")\n",
    "print(f\"Prompt B score: {score_b:.2f}\")\n",
    "print(f\"Winning prompt: {'A' if score_a > score_b else 'B'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Refinement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating response based on clarity...\n",
      "Evaluating response based on informativeness...\n",
      "Evaluating response based on engagement...\n",
      "Evaluating response based on clarity...\n",
      "Evaluating response based on informativeness...\n",
      "Evaluating response based on engagement...\n",
      "Prompt A score: 8.00\n",
      "Prompt B score: 8.00\n",
      "Winning prompt: B\n",
      "Iteration 1 prompt: Here is an updated version of the prompt template incorporating the provided feedback suggestions:\n",
      "\n",
      "**Template:**\n",
      "\n",
      "Imagine a world where computers can learn from data without being explicitly programmed. Welcome to the realm of {topic}!\n",
      "\n",
      "In traditional programming, developers tell computers exactly what to do, step by step. But in machine learning, we teach computers to figure things out on their own using examples and patterns in data.\n",
      "\n",
      "**A Simplified Example:**\n",
      "\n",
      "Let's say you want to train a computer to recognize objects or solve a problem. You start by showing it many instances of the task, like images of animals, customer interactions, or financial transactions.\n",
      "\n",
      "1. The computer analyzes the examples, identifying common characteristics or patterns.\n",
      "2. As it sees more instances, its \"brain\" becomes better at recognizing what makes something relevant to the task.\n",
      "3. When you ask it to make a decision or prediction, it can confidently provide an answer without needing explicit instructions.\n",
      "\n",
      "{Topic} works in many areas, including:\n",
      "\n",
      "* {Domain 1}: [briefly describe how {topic} is applied in this domain]\n",
      "* {Domain 2}: [briefly describe how {topic} is applied in this domain]\n",
      "* {Domain 3}: [briefly describe how {topic} is applied in this domain]\n",
      "\n",
      "By leveraging {topic}, we can develop more intelligent and adaptive systems that can learn from data, make predictions, and drive innovation.\n",
      "\n",
      "Please fill in the template with your chosen topic.\n",
      "Error in iteration 2: Missing key 'Topic'. Adjusting prompt...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Domain 1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m, in \u001b[0;36mrefine_prompt\u001b[0;34m(initial_prompt, topic, iterations)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     response \u001b[38;5;241m=\u001b[39m generate_response(\u001b[43mcurrent_prompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopic\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/genAI/.venv/lib/python3.12/site-packages/langchain_core/prompts/prompt.py:183\u001b[0m, in \u001b[0;36mPromptTemplate.format\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_partial_and_user_variables(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDEFAULT_FORMATTER_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemplate_format\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/string.py:190\u001b[0m, in \u001b[0;36mFormatter.format\u001b[0;34m(self, format_string, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/genAI/.venv/lib/python3.12/site-packages/langchain_core/utils/formatting.py:33\u001b[0m, in \u001b[0;36mStrictFormatter.vformat\u001b[0;34m(self, format_string, args, kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/string.py:194\u001b[0m, in \u001b[0;36mFormatter.vformat\u001b[0;34m(self, format_string, args, kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m used_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m--> 194\u001b[0m result, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mused_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_unused_args(used_args, args, kwargs)\n",
      "File \u001b[0;32m/usr/lib/python3.12/string.py:234\u001b[0m, in \u001b[0;36mFormatter._vformat\u001b[0;34m(self, format_string, args, kwargs, used_args, recursion_depth, auto_arg_index)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# given the field_name, find the object it references\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m#  and the argument it came from\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m obj, arg_used \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_field\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m used_args\u001b[38;5;241m.\u001b[39madd(arg_used)\n",
      "File \u001b[0;32m/usr/lib/python3.12/string.py:299\u001b[0m, in \u001b[0;36mFormatter.get_field\u001b[0;34m(self, field_name, args, kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m first, rest \u001b[38;5;241m=\u001b[39m _string\u001b[38;5;241m.\u001b[39mformatter_field_name_split(field_name)\n\u001b[0;32m--> 299\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# loop through the rest of the field_name, doing\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m#  getattr or getitem as needed\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/string.py:256\u001b[0m, in \u001b[0;36mFormatter.get_value\u001b[0;34m(self, key, args, kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Topic'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Start with the winning prompt from A/B testing\u001b[39;00m\n\u001b[1;32m     55\u001b[0m initial_prompt \u001b[38;5;241m=\u001b[39m prompt_b \u001b[38;5;28;01mif\u001b[39;00m score_b \u001b[38;5;241m>\u001b[39m score_a \u001b[38;5;28;01melse\u001b[39;00m prompt_a\n\u001b[0;32m---> 56\u001b[0m refined_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mrefine_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmachine learning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal refined prompt:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(refined_prompt\u001b[38;5;241m.\u001b[39mtemplate)\n",
      "Cell \u001b[0;32mIn[16], line 22\u001b[0m, in \u001b[0;36mrefine_prompt\u001b[0;34m(initial_prompt, topic, iterations)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Remove the problematic placeholder\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     current_prompt\u001b[38;5;241m.\u001b[39mtemplate \u001b[38;5;241m=\u001b[39m current_prompt\u001b[38;5;241m.\u001b[39mtemplate\u001b[38;5;241m.\u001b[39mreplace(\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelevant example\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m     )\n\u001b[0;32m---> 22\u001b[0m     response \u001b[38;5;241m=\u001b[39m generate_response(\u001b[43mcurrent_prompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopic\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Generate feedback and suggestions for improvement\u001b[39;00m\n\u001b[1;32m     25\u001b[0m feedback_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalyze the following explanation of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and suggest improvements to the prompt that generated it:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/genAI/.venv/lib/python3.12/site-packages/langchain_core/prompts/prompt.py:183\u001b[0m, in \u001b[0;36mPromptTemplate.format\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Format the prompt with the inputs.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m    A formatted string.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_partial_and_user_variables(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDEFAULT_FORMATTER_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemplate_format\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/string.py:190\u001b[0m, in \u001b[0;36mFormatter.format\u001b[0;34m(self, format_string, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/genAI/.venv/lib/python3.12/site-packages/langchain_core/utils/formatting.py:33\u001b[0m, in \u001b[0;36mStrictFormatter.vformat\u001b[0;34m(self, format_string, args, kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo arguments should be provided, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meverything should be passed as keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m     )\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/string.py:194\u001b[0m, in \u001b[0;36mFormatter.vformat\u001b[0;34m(self, format_string, args, kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string, args, kwargs):\n\u001b[1;32m    193\u001b[0m     used_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m--> 194\u001b[0m     result, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mused_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_unused_args(used_args, args, kwargs)\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/lib/python3.12/string.py:234\u001b[0m, in \u001b[0;36mFormatter._vformat\u001b[0;34m(self, format_string, args, kwargs, used_args, recursion_depth, auto_arg_index)\u001b[0m\n\u001b[1;32m    230\u001b[0m     auto_arg_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# given the field_name, find the object it references\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m#  and the argument it came from\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m obj, arg_used \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_field\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m used_args\u001b[38;5;241m.\u001b[39madd(arg_used)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# do any conversion on the resulting object\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/string.py:299\u001b[0m, in \u001b[0;36mFormatter.get_field\u001b[0;34m(self, field_name, args, kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_field\u001b[39m(\u001b[38;5;28mself\u001b[39m, field_name, args, kwargs):\n\u001b[1;32m    297\u001b[0m     first, rest \u001b[38;5;241m=\u001b[39m _string\u001b[38;5;241m.\u001b[39mformatter_field_name_split(field_name)\n\u001b[0;32m--> 299\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;66;03m# loop through the rest of the field_name, doing\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;66;03m#  getattr or getitem as needed\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m is_attr, i \u001b[38;5;129;01min\u001b[39;00m rest:\n",
      "File \u001b[0;32m/usr/lib/python3.12/string.py:256\u001b[0m, in \u001b[0;36mFormatter.get_value\u001b[0;34m(self, key, args, kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m args[key]\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Domain 1'"
     ]
    }
   ],
   "source": [
    "def refine_prompt(initial_prompt, topic, iterations=3):\n",
    "    \"\"\"Refine a prompt through multiple iterations.\n",
    "\n",
    "    Args:\n",
    "        initial_prompt (PromptTemplate): The starting prompt template.\n",
    "        topic (str): The topic to explain.\n",
    "        iterations (int): Number of refinement iterations.\n",
    "\n",
    "    Returns:\n",
    "        PromptTemplate: The final refined prompt template.\n",
    "    \"\"\"\n",
    "    current_prompt = initial_prompt\n",
    "    for i in range(iterations):\n",
    "        try:\n",
    "            response = generate_response(current_prompt.format(topic=topic))\n",
    "        except KeyError as e:\n",
    "            print(f\"Error in iteration {i+1}: Missing key {e}. Adjusting prompt...\")\n",
    "            # Remove the problematic placeholder\n",
    "            current_prompt.template = current_prompt.template.replace(\n",
    "                f\"{{{e.args[0]}}}\", \"relevant example\"\n",
    "            )\n",
    "            response = generate_response(current_prompt.format(topic=topic))\n",
    "\n",
    "        # Generate feedback and suggestions for improvement\n",
    "        feedback_prompt = f\"Analyze the following explanation of {topic} and suggest improvements to the prompt that generated it:\\n\\n{response}\"\n",
    "        feedback = generate_response(feedback_prompt)\n",
    "\n",
    "        # Use the feedback to refine the prompt\n",
    "        refine_prompt = f\"Based on this feedback: '{feedback}', improve the following prompt template. Ensure to only use the variable {{topic}} in your template:\\n\\n{current_prompt.template}\"\n",
    "        refined_template = generate_response(refine_prompt)\n",
    "\n",
    "        current_prompt = PromptTemplate(\n",
    "            input_variables=[\"topic\"], template=refined_template\n",
    "        )\n",
    "\n",
    "        print(f\"Iteration {i+1} prompt: {current_prompt.template}\")\n",
    "\n",
    "    return current_prompt\n",
    "\n",
    "\n",
    "# Perform A/B test\n",
    "topic = \"machine learning\"\n",
    "response_a = generate_response(prompt_a.format(topic=topic))\n",
    "response_b = generate_response(prompt_b.format(topic=topic))\n",
    "\n",
    "criteria = [\"clarity\", \"informativeness\", \"engagement\"]\n",
    "score_a = evaluate_response(response_a, criteria)\n",
    "score_b = evaluate_response(response_b, criteria)\n",
    "\n",
    "print(f\"Prompt A score: {score_a:.2f}\")\n",
    "print(f\"Prompt B score: {score_b:.2f}\")\n",
    "print(f\"Winning prompt: {'A' if score_a > score_b else 'B'}\")\n",
    "\n",
    "# Start with the winning prompt from A/B testing\n",
    "initial_prompt = prompt_b if score_b > score_a else prompt_a\n",
    "refined_prompt = refine_prompt(initial_prompt, \"machine learning\")\n",
    "\n",
    "print(\"\\nFinal refined prompt:\")\n",
    "print(refined_prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Ambiguous Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Tell me about the bank.\n",
      "The prompt \"Tell me about the bank\" is ambiguous because it lacks specificity, allowing for multiple possible interpretations. Here are some reasons why:\n",
      "\n",
      "1. **Lack of clarity on what kind of bank**: The phrase \"the bank\" could refer to a variety of institutions, such as a financial institution, a commercial bank, an online banking platform, or even a specific type of bank like a savings bank or investment bank.\n",
      "2. **Possible meanings for the word \"bank\"**: In addition to referring to a financial institution, \"bank\" can also mean a slope or incline, especially in geography (e.g., \"the river bank\") or architecture (e.g., \"a bank of buildings\").\n",
      "3. **Ambiguity about scope**: The prompt doesn't specify what aspect of the bank should be discussed. Should it be the history, services offered, products, management, policies, or something else?\n",
      "4. **Multiple possible interpretations**: Considering these factors, possible interpretations of the prompt could include:\n",
      " * Providing information about a specific commercial bank (e.g., Bank of America, Barclays)\n",
      " * Discussing online banking platforms (e.g., Chase Mobile, Bank of India)\n",
      " * Explaining the concept of a financial institution or the banking system in general\n",
      " * Describing the architecture or design of a building that is part of a bank complex\n",
      " * Offering guidance on how to navigate or use the services offered by a bank\n",
      "\n",
      "To provide a more specific and accurate response, it would be helpful if the prompt included additional context or details about what aspect of \"the bank\" should be discussed.\n",
      "--------------------------------------------------\n",
      "Prompt: What's the best way to get to school?\n",
      "The prompt \"What's the best way to get to school?\" is ambiguous because it lacks specificity regarding who is being addressed, what type of transportation is being asked about, and the context in which one might be asking this question.\n",
      "\n",
      "Here are some possible interpretations:\n",
      "\n",
      "1. **Physical presence**: The question could refer to someone physically attending a school, with various modes of transport like walking, driving, taking public transport, or biking.\n",
      "2. **Online learning**: Another interpretation is that the person is referring to an online school setting, where the mode of \"transport\" might be internet connectivity, software usage, or e-learning platforms.\n",
      "3. **School location**: The question could also imply a different meaning altogether, such as asking about how to get to a specific physical location labeled as a 'school' but which serves no educational purpose.\n",
      "\n",
      "To clarify and solve this ambiguity, additional context would be required, ensuring that the query is being asked by someone who wants to know about traveling to an actual school.\n",
      "--------------------------------------------------\n",
      "Prompt: Can you explain the theory?\n",
      "The prompt \"Can you explain the theory?\" is somewhat ambiguous because it lacks specificity about the theory in question. Here are some possible reasons for this ambiguity:\n",
      "\n",
      "1. Lack of context: The prompt does not provide any context or information about the theory being referred to, making it difficult to determine which theory the speaker is asking about.\n",
      "2. Implied generality: The use of the word \"theory\" can be interpreted as implying a broad range of possibilities, making it unclear what specific theory is being asked about.\n",
      "\n",
      "Possible interpretations of this prompt:\n",
      "\n",
      "1. **Literal interpretation**: The speaker may simply be asking for an explanation of a general concept or idea that they don't know more about.\n",
      "2. **Request for clarification**: The speaker might be seeking clarification on a particular aspect of a previously discussed theory, but without knowing what the original theory is.\n",
      "3. **Open-ended question**: The prompt could be seen as an open-ended question, asking the respondent to generate a general explanation or definition of \"theory\" itself, rather than a specific one.\n",
      "4. **Assuming a shared knowledge base**: The speaker might assume that the respondent is familiar with a particular theory and is asking for an explanation within that context.\n",
      "5. **Lack of specificity due to vagueness**: The prompt may be too vague or poorly phrased, leading to ambiguity and making it difficult for the speaker to provide a clear answer.\n",
      "\n",
      "To make the prompt more specific and reduce ambiguity, additional context or information would be necessary, such as:\n",
      "\n",
      "* \"Can you explain the [specific name of the theory]?\"\n",
      "* \"What do you think about [related topic] in terms of [theory]?\"\n",
      "* \"How does [specific concept] relate to [theoretical framework]?\"\n",
      "\n",
      "By providing more context and specificity, it becomes clearer what the speaker is asking for and can be answered with a more targeted explanation.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ambiguous_prompts = [\n",
    "    \"Tell me about the bank.\",\n",
    "    \"What's the best way to get to school?\",\n",
    "    \"Can you explain the theory?\"\n",
    "]\n",
    "\n",
    "for prompt in ambiguous_prompts:\n",
    "    analysis_prompt = f\"Analyze the following prompt for ambiguity: '{prompt}'. Explain why it's ambiguous and list possible interpretations.\"\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(llm.invoke(analysis_prompt).content)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolving Ambiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: You are a financial advisor discussing savings accounts.\n",
      "Clarified response: As a financial advisor, I'm happy to share with you the features and benefits of our institution's savings account.\n",
      "\n",
      "Our bank is a well-established, reputable financial institution that offers a range of savings options to suit various needs and goals. Our savings accounts are designed to help you build wealth over time, while also providing easy access to your funds when needed.\n",
      "\n",
      "Here are some key features of our savings account:\n",
      "\n",
      "1. **High-Yield Interest Rate**: We offer a competitive interest rate on our savings accounts, which can help you earn more interest on your deposits.\n",
      "2. **Low Fees**: Our fees are minimal and transparent, so you can keep more of your hard-earned money in your account.\n",
      "3. **Liquidity**: Our savings accounts are liquid, meaning you can access your funds whenever you need them.\n",
      "4. **Security**: Your deposits are insured by the FDIC (Federal Deposit Insurance Corporation), which protects your funds up to $250,000 per depositor, per insured bank.\n",
      "5. **Online Banking**: We offer online banking and mobile banking apps, making it easy to manage your account from anywhere.\n",
      "6. **Minimum Balance Requirements**: Some of our savings accounts come with minimum balance requirements to avoid monthly maintenance fees.\n",
      "7. **Compound Interest**: Our interest is compounded daily, which means you can earn more interest on your interest over time.\n",
      "\n",
      "Our savings accounts are designed to help you achieve your short-term and long-term financial goals, whether it's building an emergency fund, saving for a big purchase, or retirement.\n",
      "\n",
      "Which type of savings account sounds like the right fit for you?\n",
      "--------------------------------------------------\n",
      "Context: You are a geographer describing river formations.\n",
      "Clarified response: The bank of a river is a critical component in understanding its formation and behavior. The bank refers to the sloping edge or wall that forms the boundary between the river and the surrounding land.\n",
      "\n",
      "Geographers often divide the bank into two main types: the high bank and the low bank.\n",
      "\n",
      "The high bank is the steeper, more vertical side of the river valley, which is typically formed by erosion and sediment deposition. This type of bank is often characterized by a steep slope, with a relatively narrow width. The high bank can be composed of various materials, including clay, silt, and sand, depending on the local geology.\n",
      "\n",
      "The low bank, on the other hand, is the gentler, more rounded side of the river valley. This type of bank is often formed by sedimentation and aggradation, where sediments such as gravel, boulders, and vegetation accumulate over time. The low bank tends to be wider than the high bank and can exhibit a more gradual slope.\n",
      "\n",
      "The bank plays a crucial role in shaping the river's morphology and behavior, including its channel direction, meander frequency, and floodplain development. It also affects the local hydrology, with the bank influencing the flow rate, velocity, and sediment transport of the river.\n",
      "\n",
      "Understanding the relationship between the high and low banks is essential for grasping the dynamics of fluvial systems and predicting changes in river morphology over time.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def resolve_ambiguity(prompt, context):\n",
    "    \"\"\"\n",
    "    Resolve ambiguity in a prompt by providing additional context.\n",
    "    \n",
    "    Args:\n",
    "    prompt (str): The original ambiguous prompt\n",
    "    context (str): Additional context to resolve ambiguity\n",
    "    \n",
    "    Returns:\n",
    "    str: The AI's response to the clarified prompt\n",
    "    \"\"\"\n",
    "    clarified_prompt = f\"{context}\\n\\nBased on this context, {prompt}\"\n",
    "    return llm.invoke(clarified_prompt).content\n",
    "\n",
    "# Example usage\n",
    "ambiguous_prompt = \"Tell me about the bank.\"\n",
    "contexts = [\n",
    "    \"You are a financial advisor discussing savings accounts.\",\n",
    "    \"You are a geographer describing river formations.\"\n",
    "]\n",
    "\n",
    "for context in contexts:\n",
    "    print(f\"Context: {context}\")\n",
    "    print(f\"Clarified response: {resolve_ambiguity(ambiguous_prompt, context)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Techniques for Writing Clearer Prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Prompt Response:\n",
      "I'm happy to help, but I need a bit more information. You didn't specify what you're trying to make. Could you please provide more context or clarify what you're looking to create (e.g., a recipe, a craft project, a DIY item, etc.)? I'll do my best to assist you.\n",
      "\n",
      "Improved Prompt Response:\n",
      "Here's a step-by-step guide to making a classic Margherita pizza:\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 1 1/2 cups warm water\n",
      "* 1 tablespoon active dry yeast\n",
      "* 3 tablespoons olive oil\n",
      "* 1 teaspoon salt\n",
      "* 4 cups all-purpose flour (preferably \"00\" or Caputo flour)\n",
      "* 1 cup San Marzano tomatoes, crushed by hand\n",
      "* 8 ounces fresh mozzarella cheese, sliced into thin rounds\n",
      "* Fresh basil leaves\n",
      "* Extra-virgin olive oil for brushing the crust\n",
      "\n",
      "Cooking Instructions:\n",
      "\n",
      "**Step 1: Make the Pizza Dough (about 1 hour and 30 minutes)**\n",
      "\n",
      "1. In a large mixing bowl, combine warm water and yeast. Let it sit for 5-10 minutes until the yeast becomes frothy.\n",
      "2. Add olive oil, salt, and 2 cups of flour to the bowl. Mix until a shaggy dough forms.\n",
      "3. Gradually add the remaining 2 cups of flour, one cup at a time, until the dough becomes smooth and elastic.\n",
      "4. Knead the dough on a floured surface for 10-15 minutes until it becomes shiny and slightly sticky.\n",
      "5. Form the dough into a ball and place it in a lightly oiled bowl, turning the dough to coat evenly.\n",
      "6. Cover the bowl with plastic wrap or a damp towel and let the dough rise in a warm place for 1 hour, or until it has doubled in size.\n",
      "\n",
      "**Step 2: Prepare the Tomato Sauce (about 15 minutes)**\n",
      "\n",
      "1. Crush the San Marzano tomatoes by hand using a spoon or the back of a fork.\n",
      "2. Season the tomato sauce with salt and a pinch of sugar to balance the acidity.\n",
      "\n",
      "**Step 3: Shape the Pizza Dough (about 10 minutes)**\n",
      "\n",
      "1. Preheat your oven to 500°F (260°C) with a pizza stone or baking sheet inside, if you have one.\n",
      "2. Punch down the risen dough and divide it into 2-4 equal portions, depending on the size of pizza you prefer.\n",
      "3. Roll out each portion into a thin circle, about 12 inches in diameter.\n",
      "\n",
      "**Step 4: Assemble the Pizza (about 10 minutes)**\n",
      "\n",
      "1. Transfer the rolled-out dough to a lightly floured surface or a piece of parchment paper.\n",
      "2. Spread a thin layer of tomato sauce over the dough, leaving a 1/2-inch border around the edges.\n",
      "3. Arrange sliced mozzarella cheese on top of the sauce, overlapping them slightly.\n",
      "4. Drizzle with extra-virgin olive oil and sprinkle with fresh basil leaves.\n",
      "\n",
      "**Step 5: Bake the Pizza (about 10-15 minutes)**\n",
      "\n",
      "1. Slide the pizza onto the preheated stone or baking sheet.\n",
      "2. Bake for 10-15 minutes until the crust is golden brown, the cheese is melted, and the sauce is bubbly.\n",
      "3. Remove the pizza from the oven and let it cool for a few minutes before slicing and serving.\n",
      "\n",
      "Tips:\n",
      "\n",
      "* Use high-quality ingredients, especially San Marzano tomatoes and fresh mozzarella cheese.\n",
      "* Make sure the dough has risen evenly and has a smooth texture.\n",
      "* Preheat your oven to its highest temperature possible to achieve a crispy crust.\n",
      "* Don't overload the pizza with toppings, as this can make it difficult to cook evenly.\n",
      "\n",
      "Enjoy your delicious homemade Margherita pizza!\n"
     ]
    }
   ],
   "source": [
    "def compare_prompt_clarity(original_prompt, improved_prompt):\n",
    "    \"\"\"\n",
    "    Compare the responses to an original prompt and an improved, clearer version.\n",
    "    \n",
    "    Args:\n",
    "    original_prompt (str): The original, potentially unclear prompt\n",
    "    improved_prompt (str): An improved, clearer version of the prompt\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Responses to the original and improved prompts\n",
    "    \"\"\"\n",
    "    original_response = llm.invoke(original_prompt).content\n",
    "    improved_response = llm.invoke(improved_prompt).content\n",
    "    return original_response, improved_response\n",
    "\n",
    "# Example usage\n",
    "original_prompt = \"How do I make it?\"\n",
    "improved_prompt = \"Provide a step-by-step guide for making a classic margherita pizza, including ingredients and cooking instructions.\"\n",
    "\n",
    "original_response, improved_response = compare_prompt_clarity(original_prompt, improved_prompt)\n",
    "\n",
    "print(\"Original Prompt Response:\")\n",
    "print(original_response)\n",
    "print(\"\\nImproved Prompt Response:\")\n",
    "print(improved_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Prompts for Clarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided aspects, here is an analysis of the impact of social media on society:\n",
      "\n",
      "1. **Mental Health and Well-being:**\n",
      "\n",
      "Social media has been shown to have both positive and negative effects on mental health and well-being. On the one hand, social media platforms can provide a sense of community and connection for people who may be isolated or struggling with loneliness. Social support groups, online forums, and social media communities can offer emotional support, advice, and a sense of belonging. Additionally, social media can also be used to promote mental health awareness, reduce stigma around mental illness, and provide access to mental health resources.\n",
      "\n",
      "On the other hand, excessive social media use has been linked to increased symptoms of depression, anxiety, and loneliness. The constant stream of curated and often unrealistic content can create unrealistic expectations and promote a culture of comparison, where individuals feel inadequate or insufficient compared to others' online personas. Furthermore, the lack of face-to-face interaction and deep conversations on social media can lead to feelings of isolation and disconnection.\n",
      "\n",
      "2. **Social Relationships and Community:**\n",
      "\n",
      "Social media has transformed the way we form and maintain relationships. While it provides an opportunity for people to connect with others across geographical boundaries, it also poses challenges for traditional social interactions. Social media platforms have enabled the creation of online communities centered around shared interests, identities, or causes, which can foster a sense of belonging and connection.\n",
      "\n",
      "However, excessive social media use has been linked to decreased face-to-face interaction, deepened social isolation, and reduced empathy. The algorithm-driven nature of many social media platforms creates an echo chamber effect, where individuals are only exposed to content that aligns with their existing views and interests, further reinforcing existing biases and limiting exposure to diverse perspectives.\n",
      "\n",
      "3. **Information Dissemination and Civic Engagement:**\n",
      "\n",
      "Social media has revolutionized the way we consume and disseminate information. Platforms like Twitter, Facebook, and Instagram have enabled rapid sharing of news, opinions, and ideas, allowing for real-time discussions and debates on pressing issues. Social media has also provided a unique platform for citizen journalism, social activism, and grassroots organizing.\n",
      "\n",
      "However, the spread of misinformation, disinformation, and propaganda on social media platforms has significant implications for civic engagement and democratic processes. The lack of fact-checking, media literacy, and critical thinking skills among some users can contribute to the amplification of false or misleading information, eroding trust in institutions and undermining democratic discourse.\n",
      "\n",
      "In conclusion, the impact of social media on society is complex and multifaceted. While it has the potential to enhance our social connections, mental health, and civic engagement, excessive use and algorithmic design issues also pose significant challenges. A balanced approach that promotes responsible social media usage, digital literacy, and critical thinking can help mitigate these negative effects and harness the full potential of social media for the betterment of society.\n"
     ]
    }
   ],
   "source": [
    "structured_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"aspects\", \"tone\"],\n",
    "    template=\"\"\"Provide an analysis of {topic} considering the following aspects:\n",
    "    1. {{aspects[0]}}\n",
    "    2. {{aspects[1]}}\n",
    "    3. {{aspects[2]}}\n",
    "    \n",
    "    Present the analysis in a {tone} tone.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "input_variables = {\n",
    "    \"topic\": \"the impact of social media on society\",\n",
    "    \"aspects\": [\"communication patterns\", \"mental health\", \"information spread\"],\n",
    "    \"tone\": \"balanced and objective\"\n",
    "}\n",
    "\n",
    "chain = structured_prompt | llm\n",
    "response = chain.invoke(input_variables).content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: What's the difference?\n",
      "Improved: What are the key differences between two or more concepts, ideas, or things that you would like me to compare?\n",
      "--------------------------------------------------\n",
      "Original: How does it work?\n",
      "Improved: What are the underlying mechanics or principles behind this system/component/technology?\n",
      "--------------------------------------------------\n",
      "Original: Why is it important?\n",
      "Improved: What are the potential consequences or benefits of [specific topic or action]?\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "unclear_prompts = [\n",
    "    \"What's the difference?\",\n",
    "    \"How does it work?\",\n",
    "    \"Why is it important?\"\n",
    "]\n",
    "\n",
    "def improve_prompt_clarity(unclear_prompt):\n",
    "    \"\"\"\n",
    "    Improve the clarity of a given prompt.\n",
    "    \n",
    "    Args:\n",
    "    unclear_prompt (str): The original unclear prompt\n",
    "    \n",
    "    Returns:\n",
    "    str: An improved, clearer version of the prompt\n",
    "    \"\"\"\n",
    "    improvement_prompt = f\"The following prompt is unclear: '{unclear_prompt}'. Please provide a clearer, more specific version of this prompt. output just the improved prompt and nothing else.\" \n",
    "    return llm.invoke(improvement_prompt).content\n",
    "\n",
    "for prompt in unclear_prompts:\n",
    "    improved_prompt = improve_prompt_clarity(prompt)\n",
    "    print(f\"Original: {prompt}\")\n",
    "    print(f\"Improved: {improved_prompt}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing Prompt Length and Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed response:\n",
      "Artificial intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence, such as learning, problem-solving, decision-making, and perception. AI involves the creation of algorithms and statistical models that enable machines to analyze data, recognize patterns, and make predictions or decisions.\n",
      "\n",
      "Historical Context:\n",
      "\n",
      "The concept of AI dates back to ancient Greece, where myths told of artificial beings created by the gods. However, the modern concept of AI emerged in the mid-20th century with the work of Alan Turing, who proposed the Turing Test as a measure of a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human.\n",
      "\n",
      "In the 1950s and 1960s, AI research focused on rule-based systems and expert systems. The first AI program, called Logical Theorist, was developed in 1956 by Allen Newell and Herbert Simon. However, it wasn't until the 1980s that AI began to gain momentum with the development of machine learning algorithms and neural networks.\n",
      "\n",
      "Key Components:\n",
      "\n",
      "1. **Machine Learning**: A type of AI that enables machines to learn from data without being explicitly programmed.\n",
      "2. **Natural Language Processing (NLP)**: The ability of computers to understand, generate, and process human language.\n",
      "3. **Computer Vision**: The ability of computers to interpret and make decisions based on visual data from images or videos.\n",
      "4. **Robotics**: The use of AI in robots that can interact with their environment and perform tasks.\n",
      "5. **Deep Learning**: A type of machine learning that uses neural networks to analyze data.\n",
      "\n",
      "Practical Applications:\n",
      "\n",
      "1. **Virtual Assistants**: AI-powered virtual assistants, such as Siri, Alexa, and Google Assistant, are used in smartphones, smart speakers, and other devices.\n",
      "2. **Image Recognition**: AI is used in self-driving cars to recognize objects, people, and road signs.\n",
      "3. **Chatbots**: AI-powered chatbots are used in customer service, customer support, and online marketing.\n",
      "4. **Predictive Maintenance**: AI is used in industries such as manufacturing, transportation, and energy to predict equipment failures and maintenance needs.\n",
      "5. **Healthcare**: AI is used in medical imaging, disease diagnosis, and personalized medicine.\n",
      "\n",
      "Examples:\n",
      "\n",
      "1. **Google's AlphaGo**: A computer program that defeated a human world champion in Go in 2016.\n",
      "2. **Amazon's Alexa**: A virtual assistant that can answer questions, play music, and control smart home devices.\n",
      "3. **Self-Driving Cars**: Companies such as Waymo and Tesla are developing self-driving cars using AI and machine learning algorithms.\n",
      "\n",
      "Controversies and Debates:\n",
      "\n",
      "1. **Job Displacement**: The impact of AI on employment and the potential displacement of jobs.\n",
      "2. **Bias and Fairness**: Concerns about bias in AI systems, particularly in areas such as facial recognition and hiring.\n",
      "3. **Explainability**: The lack of transparency and explainability in AI decision-making processes.\n",
      "4. **Data Privacy**: Concerns about data privacy and the potential for AI to be used for surveillance and manipulation.\n",
      "\n",
      "Future Developments and Trends:\n",
      "\n",
      "1. **Edge AI**: The use of AI on edge devices, such as smartphones and smart home devices, rather than relying on cloud-based processing.\n",
      "2. **Explainable AI (XAI)**: Developing methods to explain and interpret the decisions made by AI systems.\n",
      "3. **Transfer Learning**: Using pre-trained models to adapt to new tasks and domains.\n",
      "4. **Human-AI Collaboration**: Designing AI systems that can collaborate with humans in more effective ways.\n",
      "\n",
      "In conclusion, artificial intelligence has come a long way since its inception, and it continues to evolve at an incredible pace. As AI becomes increasingly integrated into our daily lives, we must address the controversies and debates surrounding its development and ensure that its benefits are shared by all.\n",
      "\n",
      "Concise response:\n",
      "Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. The main goal of AI is to create machines that can perform tasks that typically require human intelligence, such as:\n",
      "\n",
      "1. Learning: AI systems can learn from data and improve their performance over time.\n",
      "2. Problem-solving: AI systems can analyze data, identify patterns, and make decisions based on that analysis.\n",
      "3. Perception: AI systems can interpret and understand data from sensors, such as images and speech.\n",
      "\n",
      "The main importance of AI is:\n",
      "\n",
      "1. **Improved Efficiency**: AI can automate repetitive tasks, freeing up humans to focus on more complex and creative work.\n",
      "2. **Enhanced Decision-making**: AI can analyze large amounts of data, providing insights that humans might miss.\n",
      "3. **Increased Productivity**: AI can help businesses streamline processes, improve customer service, and reduce costs.\n",
      "4. **Innovation**: AI has the potential to transform industries such as healthcare, finance, and transportation, leading to new business models and opportunities.\n",
      "\n",
      "Overall, AI is becoming increasingly important in various aspects of life, from personal assistance to complex decision-making, and its applications continue to expand and evolve.\n"
     ]
    }
   ],
   "source": [
    "# Detailed prompt\n",
    "detailed_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"\"\"Please provide a comprehensive explanation of {topic}. Include its definition, \n",
    "    historical context, key components, practical applications, and any relevant examples. \n",
    "    Also, discuss any controversies or debates surrounding the topic, and mention potential \n",
    "    future developments or trends.\"\"\"\n",
    ")\n",
    "\n",
    "# Concise prompt\n",
    "concise_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Briefly explain {topic} and its main importance.\"\n",
    ")\n",
    "\n",
    "topic = \"artificial intelligence\"\n",
    "\n",
    "print(\"Detailed response:\")\n",
    "print(llm.invoke(detailed_prompt.format(topic=topic)).content)\n",
    "\n",
    "print(\"\\nConcise response:\")\n",
    "print(llm.invoke(concise_prompt.format(topic=topic)).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Prompt Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Differences Analysis**\n",
      "\n",
      "1. **Information Coverage**:\n",
      "\t* Detailed Response: Provides a comprehensive overview of AI, including its definition, historical context, key components, practical applications, examples, controversies, and future developments.\n",
      "\t* Concise Response: Focuses on the core aspects of AI, such as automation, decision-making, customer experience, innovation, and efficiency.\n",
      "2. **Clarity and Focus**:\n",
      "\t* Detailed Response: Explains complex concepts like machine learning, deep learning, and natural language processing in detail, making it suitable for an audience with a technical background or interested in the intricacies of AI.\n",
      "\t* Concise Response: Simplifies these concepts and focuses on their practical applications, making it more accessible to a broader audience.\n",
      "3. **Potential Use Cases**:\n",
      "\t* Detailed Response: Suitable for educational purposes, research papers, or articles targeting an audience with a technical background or interested in the theoretical aspects of AI.\n",
      "\t* Concise Response: Ideal for marketing materials, briefings, or presentations aimed at a general audience looking to understand the benefits and applications of AI.\n",
      "\n",
      "**Balancing Detail and Conciseness**\n",
      "\n",
      "To balance detail and conciseness in prompts, consider the following strategies:\n",
      "\n",
      "1. **Define the purpose**: Clearly define the purpose of the response, including the target audience and the desired level of detail.\n",
      "2. **Use a tiered approach**: Divide complex topics into smaller, more manageable chunks, allowing for a detailed explanation or concise summary, depending on the requirements.\n",
      "3. **Focus on key points**: Identify the most critical information and prioritize it in the response, ensuring that the most important details are covered while keeping the rest concise.\n",
      "4. **Use visual aids**: Incorporate diagrams, charts, or infographics to help illustrate complex concepts and make them more accessible.\n",
      "5. **Consider the audience's level of expertise**: Tailor the language, tone, and level of detail to suit the audience's background knowledge and understanding.\n",
      "6. **Keep it concise in mind**: Strive for a clear structure, concise paragraphs, and minimal unnecessary information to maintain readability and engagement.\n",
      "\n",
      "By considering these strategies, you can create responses that strike a balance between providing detailed explanations and delivering concise, easily digestible information.\n"
     ]
    }
   ],
   "source": [
    "analysis_prompt = PromptTemplate(\n",
    "    input_variables=[\"detailed_response\", \"concise_response\"],\n",
    "    template=\"\"\"Compare the following two responses on artificial intelligence:\n",
    "\n",
    "Detailed response:\n",
    "{detailed_response}\n",
    "\n",
    "Concise response:\n",
    "{concise_response}\n",
    "\n",
    "Analyze the differences in terms of:\n",
    "1. Information coverage\n",
    "2. Clarity and focus\n",
    "3. Potential use cases for each type of response\n",
    "\n",
    "Then, suggest strategies for balancing detail and conciseness in prompts.\"\"\"\n",
    ")\n",
    "\n",
    "detailed_response = llm.invoke(detailed_prompt.format(topic=topic)).content\n",
    "concise_response = llm.invoke(concise_prompt.format(topic=topic)).content\n",
    "\n",
    "analysis = llm.invoke(analysis_prompt.format(\n",
    "    detailed_response=detailed_response,\n",
    "    concise_response=concise_response\n",
    ")).content\n",
    "\n",
    "print(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process-Flow for Handling Long Contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 2\n",
      "First chunk: Artificial intelligence (AI) is a branch of computer science that aims to create intelligent machines that can simulate human cognitive processes.\n",
      "The field of AI has a rich history dating back to the...\n"
     ]
    }
   ],
   "source": [
    "long_text = \"\"\"\n",
    "Artificial intelligence (AI) is a branch of computer science that aims to create intelligent machines that can simulate human cognitive processes.\n",
    "The field of AI has a rich history dating back to the 1950s, with key milestones such as the development of the first neural networks and expert systems.\n",
    "AI encompasses a wide range of subfields, including machine learning, natural language processing, computer vision, and robotics.\n",
    "Practical applications of AI include speech recognition, image classification, autonomous vehicles, and medical diagnosis.\n",
    "AI has the potential to revolutionize many industries, from healthcare and finance to transportation and entertainment.\n",
    "However, there are ongoing debates and controversies surrounding AI, such as concerns about job displacement, bias in algorithms, and the ethical implications of autonomous systems.\n",
    "Looking ahead, the future of AI holds promise for advancements in areas like explainable AI, AI ethics, and human-AI collaboration. \n",
    "The intersection of AI with other technologies like blockchain, quantum computing, and biotechnology will likely shape the future of the field.\n",
    "But as AI continues to evolve, it is essential to consider the societal impact and ethical implications of these technologies.\n",
    "One of the key challenges for AI researchers and developers is to strike a balance between innovation and responsibility, ensuring that AI benefits society as \n",
    "a whole while minimizing potential risks.\n",
    "If managed effectively, AI has the potential to transform our world in ways we can only begin to imagine.\n",
    "Though the future of AI is uncertain, one thing is clear: the impact of artificial intelligence will be profound and far-reaching.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Split the text into chunks\n",
    "chunks = text_splitter.split_text(long_text)\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"First chunk: {chunks[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalki/genAI/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Artificial intelligence (AI) is a branch of computer science aimed at creating intelligent machines, with numerous subfields and practical applications. While it holds promise for revolutionizing industries, it also raises concerns about job displacement, bias, and ethics. The future of AI promises advancements in explainability, ethics, and human-AI collaboration, but requires careful consideration of societal implications to unlock its full potential.\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Convert text chunks to Document objects\n",
    "doc_chunks = [Document(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "# Load the summarization chain\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "\n",
    "# Summarize the long text\n",
    "summary_result = chain.invoke(doc_chunks)\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(summary_result['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Iterative Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Analysis:\n",
      "Based on the analysis, it appears that the text provides a general overview of Artificial Intelligence (AI) and its various aspects. The summary sections highlight the definition, history, scope, applications, challenges, and future developments of AI. However, the text lacks specific details and concrete examples to support these points.\n",
      "\n",
      "One notable aspect is the emphasis on the potential benefits and challenges of AI, highlighting its transformative impact on industries and society as a whole. The text also acknowledges the importance of considering ethical implications, societal implications, and responsible development in the field of AI.\n",
      "\n",
      "The analysis reveals that the text appears to be more of an introduction or overview of AI, rather than a comprehensive treatment of the subject. The lack of specific examples and concrete data suggests that the text may not be intended for an academic or technical audience seeking in-depth knowledge on the topic.\n",
      "\n",
      "In conclusion, while the text provides a general framework for understanding AI, it lacks specificity and depth, suggesting that it may be intended as a introductory resource rather than a comprehensive treatment of the subject.\n"
     ]
    }
   ],
   "source": [
    "def iterative_analysis(text, steps):\n",
    "    \"\"\"\n",
    "    Perform iterative analysis on a given text.\n",
    "    \n",
    "    Args:\n",
    "    text (str): The text to analyze.\n",
    "    steps (list): List of analysis steps to perform.\n",
    "    \n",
    "    Returns:\n",
    "    str: The final analysis result.\n",
    "    \"\"\"\n",
    "    result = text\n",
    "    for step in steps:\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=f\"Analyze the following text. {step}\\n\\nText: {{text}}\\n\\nAnalysis:\"\n",
    "        )\n",
    "        result = llm.invoke(prompt.format(text=result)).content\n",
    "    return result\n",
    "\n",
    "analysis_steps = [\n",
    "    \"Identify the main topics discussed.\",\n",
    "    \"Summarize the key points for each topic.\",\n",
    "    \"Provide a brief conclusion based on the analysis.\"\n",
    "]\n",
    "\n",
    "final_analysis = iterative_analysis(long_text, analysis_steps)\n",
    "print(\"Final Analysis:\")\n",
    "print(final_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
